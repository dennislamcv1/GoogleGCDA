{"cells":[{"cell_type":"markdown","id":"8207e923-2b0e-4c77-8af9-42e121e9aa17","metadata":{},"source":["# Query store data with Spark SQL"]},{"cell_type":"markdown","id":"70c89537-28a8-41b6-9941-41dc2a8122ea","metadata":{},"source":["### 1) Create a Spark session"]},{"cell_type":"code","execution_count":1,"id":"bc60ab9a-cd91-439c-ba6e-3223e2abc1b2","metadata":{"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","24/07/15 13:02:34 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n","24/07/15 13:02:34 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n","24/07/15 13:02:34 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n","24/07/15 13:02:34 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n"]}],"source":["from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder \\\n","  .appName(\"Physical Store Returns Processing\")\\\n","  .master(\"yarn\")\\\n","  .enableHiveSupport()\\\n","  .getOrCreate()"]},{"cell_type":"code","execution_count":2,"id":"c22a46d3-1df4-456d-be91-29de1922d795","metadata":{},"outputs":[{"data":{"text/html":["\n","            <div>\n","                <p><b>SparkSession - hive</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://mycluster-m.us-east1-c.c.qwiklabs-gcp-04-c6ac3d8e53ab.internal:34823\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.1.3</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>yarn</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>Physical Store Returns Processing</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "],"text/plain":["<pyspark.sql.session.SparkSession at 0x7fe215e585b0>"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["# Display spark session properties\n","spark"]},{"cell_type":"markdown","id":"09e1031e-ab0f-46b5-80d4-e3801e30b361","metadata":{},"source":["### 2) Read the combined returns data from the Parquet file"]},{"cell_type":"code","execution_count":3,"id":"189a4327-f028-4e29-8531-7de006a0f12b","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# Retrieve the current project id and store in the variable called PROJECT_ID\n","PROJECT_ID=!gcloud info --format='value(config.project)'\n","\n","# Read data from Parquet files in Google Cloud Storage and populate a DataFrame\n","# The Google Cloud Storage bucket name in this lab is the name of the project id\n","# We are using the project_id value obtained in the prior step in the gs:// path\n","store_returns_with_addr_dataframe = spark.read.parquet(\"gs://\" + PROJECT_ID[0] + \"/store_returns_output/store_returns.parquet/*.parquet\")\n"]},{"cell_type":"code","execution_count":4,"id":"1ec18fc5-a69a-4f14-a55f-b0ed94ffa575","metadata":{"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- order_id: integer (nullable = true)\n"," |-- rma_id: integer (nullable = true)\n"," |-- return_status: string (nullable = true)\n"," |-- status_date: string (nullable = true)\n"," |-- product_id: integer (nullable = true)\n"," |-- quantity_returned: integer (nullable = true)\n"," |-- store_id: integer (nullable = true)\n"," |-- street_address: string (nullable = true)\n","\n"]}],"source":["# Print schema to review field names and data types\n","store_returns_with_addr_dataframe.printSchema()"]},{"cell_type":"code","execution_count":5,"id":"59e0224c-44ad-4345-a3ce-4ef98460f11f","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 1:>                                                          (0 + 1) / 1]\r"]},{"name":"stdout","output_type":"stream","text":["+--------+------+-------------+-----------------------+----------+-----------------+--------+---------------------------+\n","|order_id|rma_id|return_status|status_date            |product_id|quantity_returned|store_id|street_address             |\n","+--------+------+-------------+-----------------------+----------+-----------------+--------+---------------------------+\n","|44176   |39613 |in progress  |null                   |25059     |1                |20000   |000 Amber Viaduct Suite 499|\n","|16558   |1440  |complete     |2020-03-18 13:46:00 UTC|8875      |1                |20000   |000 Amber Viaduct Suite 499|\n","|29503   |34936 |complete     |null                   |3883      |1                |20000   |000 Amber Viaduct Suite 499|\n","+--------+------+-------------+-----------------------+----------+-----------------+--------+---------------------------+\n","only showing top 3 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"data":{"text/plain":["None"]},"metadata":{},"output_type":"display_data"}],"source":["display(store_returns_with_addr_dataframe.show(3,truncate=False))"]},{"cell_type":"code","execution_count":6,"id":"c410e896-7d95-4c5c-addc-3f9098e2211f","metadata":{},"outputs":[],"source":["# Create a view so that the DataFrame store_returns_with_address can be referenced as a table in Spark SQL\n","store_returns_with_addr_dataframe.createOrReplaceTempView(\"store_returns_with_address\")"]},{"cell_type":"code","execution_count":7,"id":"590cfb9a-b9c8-4a2d-8fa2-4a4bb071c916","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------+------+-------------+-----------------------+----------+-----------------+--------+---------------------------+\n","|order_id|rma_id|return_status|status_date            |product_id|quantity_returned|store_id|street_address             |\n","+--------+------+-------------+-----------------------+----------+-----------------+--------+---------------------------+\n","|44176   |39613 |in progress  |null                   |25059     |1                |20000   |000 Amber Viaduct Suite 499|\n","|16558   |1440  |complete     |2020-03-18 13:46:00 UTC|8875      |1                |20000   |000 Amber Viaduct Suite 499|\n","|29503   |34936 |complete     |null                   |3883      |1                |20000   |000 Amber Viaduct Suite 499|\n","+--------+------+-------------+-----------------------+----------+-----------------+--------+---------------------------+\n","only showing top 3 rows\n","\n"]}],"source":["# Show store returns with address using Spark SQL\n","spark.sql(\"select * from store_returns_with_address\").show(3, truncate=False)"]},{"cell_type":"markdown","id":"7891bee0-848b-4e72-80d4-b5ea9066dd6d","metadata":{},"source":["### 3) Returns by month using Spark SQL\n","\n","In this section you will run a query that generates the same results you have seen in BigQuery before, but processed by Dataproc Spark and executed in a Jupyter Notebook.\n","\n","The temporary view store_returns_with_address in the \"from\" line references a Spark DataFrame that contains data from the Parquet file in Google Cloud Storage. In BigQuery, the \"from\" line references a BigQuery standard table."]},{"cell_type":"code","execution_count":8,"id":"3fe366db-2251-4014-add8-3e5076da0167","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 3:>                                                          (0 + 1) / 1]\r"]},{"name":"stdout","output_type":"stream","text":["+----------+-------------+-----------+\n","|year_month|return_status|order_count|\n","+----------+-------------+-----------+\n","|   2020-07|      unknown|          6|\n","|   2021-05|     complete|         67|\n","|   2019-07|      unknown|          1|\n","|   2022-01|      unknown|         18|\n","|   2022-10|  in progress|         44|\n","|   2020-06|      unknown|          7|\n","|   2019-12|      unknown|          2|\n","|   2022-03|      unknown|         24|\n","|   2023-02|      unknown|         44|\n","|   2019-06|      unknown|          1|\n","|   2023-02|     complete|        197|\n","|   2021-04|      unknown|         15|\n","|   2023-03|      unknown|         50|\n","|   2019-05|     complete|          6|\n","|   2020-01|  in progress|         11|\n","|   2023-06|      unknown|         43|\n","|   2021-02|      unknown|         10|\n","|   2021-08|     complete|         76|\n","|   2019-09|  in progress|          3|\n","|   2022-10|      unknown|         40|\n","+----------+-------------+-----------+\n","only showing top 20 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# Test the query you ran in BigQuery\n","dataframe = spark.sql('''\n","    select\n","    substring(status_date, 1, 7) as year_month,\n","    return_status,\n","    count(order_id) as order_count\n","    from store_returns_with_address\n","    group by year_month, return_status\n","''')\n","dataframe.show()"]},{"cell_type":"markdown","id":"93adf4a1-9948-46ba-a198-c1e898c07458","metadata":{},"source":["### Return to the lab instructions"]},{"cell_type":"code","execution_count":null,"id":"0304fba1-bef1-404a-96cc-3c910e1900e3","metadata":{},"outputs":[],"source":[]}],"metadata":{"environment":{"kernel":"python3","name":"tf2-cpu.2-11.m112","type":"gcloud","uri":"gcr.io/deeplearning-platform-release/tf2-cpu.2-11:m112"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.15"},"serverless_spark":"{\"name\":\"projects/charaka-349315/locations/us-central1/sessions/charaka-vai\",\"uuid\":\"42dba073-472b-473d-ba31-d3668c0209d8\",\"createTime\":\"2022-05-12T06:24:06.691633Z\",\"jupyterSession\":{},\"spark\":{},\"runtimeInfo\":{\"endpoints\":{\"Spark History Server\":\"https://igxqev2rzjf67bpo3ex3bgyse4-dot-us-central1.dataproc.googleusercontent.com/sparkhistory/\"},\"outputUri\":\"https://nt2x34fyjrgivaygsqhqmzdxca-dot-us-central1.dataproc.googleusercontent.com/gateway/default/jupyter/lab/\"},\"state\":\"ACTIVE\",\"stateTime\":\"2022-05-12T06:25:15.559667Z\",\"creator\":\"s8s-lab-sa@charaka-349315.iam.gserviceaccount.com\",\"runtimeConfig\":{\"properties\":{\"spark:spark.jars\":\"gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.22.2.jar\",\"spark:spark.executor.instances\":\"2\",\"spark:spark.driver.cores\":\"4\",\"spark:spark.executor.cores\":\"4\",\"spark:spark.eventLog.dir\":\"gs://s8s-sphs-159504796045/42dba073-472b-473d-ba31-d3668c0209d8/spark-job-history\"}},\"environmentConfig\":{\"executionConfig\":{\"serviceAccount\":\"s8s-lab-sa@charaka-349315.iam.gserviceaccount.com\",\"subnetworkUri\":\"https://www.googleapis.com/compute/v1/projects/charaka-349315/regions/us-central1/subnetworks/spark-snet\"},\"peripheralsConfig\":{\"sparkHistoryServerConfig\":{\"dataprocCluster\":\"projects/charaka-349315/regions/us-central1/clusters/s8s-sphs-159504796045\"}}}}","serverless_spark_kernel_name":"remote-bde5aa2c167788804d4b36b2-pyspark"},"nbformat":4,"nbformat_minor":5}